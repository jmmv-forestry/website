---
title: "Compiler-level parallelization and languages"
date: 2007-06-09 12:09:00 -0400
categories:
  - "compiler"
  - "haskell"
  - "multicore"
  - "optimization"
  - "parallelism"
julipedia: 2007/06/compiler-level-parallelization-and.html
slug: compiler-level-parallelization-and
---
Some days ago, Intel <a href="http://www.intel.com/pressroom/archive/releases/20070605comp_a.htm?iid=pr1_releasepri_20070605ma">announced a new version</a> of their C++ and Fortran compilers.  According to their announcement:<br /><blockquote>Application performance is also accelerated by multi-core processors through the use of multiple threads.</blockquote>So... as far as I understand, and as some other news sites mention, this means that the compiler tries to automatically parallelize a program by creating multiple threads; the code executed on each thread is decided at build time through some algorithm that deduces which blocks of code can be executed at the same time.<br /><br />If this is true — I mean, if I understood it correctly —, it sounds great but I don't know to what level the compiler is able to extract useful information from code blocks in either C++ and Fortran.  These two languages follow the imperative programming paradigm: a program written in them describes <span style="font-style: italic;">step by step</span> how the machine must operate.  In other words: the program specifies how a specific type of machine (a load/store one) must behave in order to compute a result, not how the result is computed.<br /><br />Extracting parallelization information from this type of languages seems hard if not impossible except for very simple cases.  Even more, most imperative languages are not protected against side effects: there is a global state that is accessible from any part of the program, which means that you cannot predict how a specific call will change this global state.  In terms of functions: a function with a specific set of parameters can return different values on each call, because it can store auxiliary information on global variables.<br /><br />It seems to me that functional languages are much more suitable to this kind of compiler-level parallelization than imperative ones.  In a functional language, the program describes how to compute a result at an abstract level, not how to reach that result by a specific type of machine.  The way the compiler arrives to that result is generally irrelevant.  (If you know SQL, it has the same properties: you describe what you want to know through a query but you don't know how the database engine will handle it.)  Furthermore, and this is important, purely functional languages such as <a href="http://www.haskell.org/">Haskell</a> do not have side effects as long as you don't use monads.  So what does this mean? A function, when called with a specific same of parameters, will <i>always</i> return the same result.  (So yes, the compiler could, and possible does, trivially apply <a href="http://en.wikipedia.org/wiki/Memoization">memoization</a>.)<br /><br />With these characteristics, a compiler for a functional language could do much more to implement compiler-level parallelization.  Each call to a function could be analyzed to see which other functions it calls, thus generating a call graph; later on, the compiler could decide which subset of this graph is sent to each computing core (i.e. placed on an independent thread) and merge the results between threads when it got to processing the top level function it split.  So if you had an expression such as:<br /><br /><tt>foo = (bar 3 4) + (baz 5 6)</tt><br /><br />The compiler could prepare two threads, one to compute the result of <tt>bar 3 4</tt> and one to calculate <tt>bar 5 6</tt>.  At the end, and after the two threads finished, it could do the sum.  Of course, <tt>bar</tt> and <tt>baz</tt> could have to be "large" enough to compensate the time spent on creating and managing the threads.<br /><br />Anyway, what I wanted to emphasize is that depending on the language you choose, doing specific types of code analysis and optimization can be much easier and, of course, much better.<br /><br />To conclude, and as I'm talking about Haskell, I'd like to suggest you to read the article "<a href="http://www.onlamp.com/pub/a/onlamp/2007/05/21/an-introduction-to-haskell---part-1-why-haskell.html">An introduction to Haskell, part 1</a>" recently published <a href="http://www.onlamp.com/">at ONLamp.com</a>.  It ends talking about this idea a bit.</tt>
